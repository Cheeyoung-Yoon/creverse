
# 📑 Essay Evaluation Project Report

## 1. 프로젝트 개요
- 목적: 학생 에세이 자동 평가 시스템 구현
- 주요 요구사항:
  - Prompt Ops 기반 버전 관리
  - FastAPI 비동기 API
  - Rubric 기반 자동 평가 로직
  - Langfuse Trace 기록 및 분석
- 사용 모델: Azure OpenAI `azure openai gpt-5-mini`

---


## 2. 데이터 및 평가 기준

* 입력 데이터: 40개 (레벨별 10개씩, Basic/Intermediate/Advanced/Expert)
* 평가 Rubric:

  * 서론 / 본론 / 결론 / 문법 (각 0~2점)

---
## 3. 실험 환경

- FastAPI 기반 API 서비스
- Prompt Ops 기반 버전 관리 (v1.0.0 ~ v1.5.0)
- Langfuse를 통한 LLM 호출 로그 및 추적 관리
- 주요 버전: v1.5.0 (최종)


## 4. 결과 분석

### 4.1 레벨 그룹별 변화

* 동일 데이터셋을 평가했을 때 레벨 기준이 올라감에 따라 점수가 점차 하락

| Level        | Prompt_Version | Total_Essays | Successful_Calls | Failed_Calls | Success_Rate | Avg_Response_Time | Avg_Introduction | Avg_Body | Avg_Conclusion | Avg_Grammar |
|--------------|----------------|--------------|------------------|--------------|--------------|-------------------|------------------|----------|----------------|-------------|
| Basic        | v1.5.0         | 40           | 40               | 0            | 100.0%       | 31.86s            | 1.95             | 2.000    | 1.575          | 1.45        |
| Intermediate | v1.5.0         | 40           | 40               | 0            | 100.0%       | 34.86s            | 1.325            | 1.050    | 0.925          | 1.025       |
| Advanced     | v1.5.0         | 40           | 38               | 2            | 95.0%        | 30.70s            | 0.895            | 0.842    | 0.816          | 0.316       |
| Expert       | v1.5.0         | 40           | 39               | 1            | 97.5%        | 37.72s            | 0.154            | 0.103    | 0.026          | 0.615       |


- Basic 으로 채점 시, 전반적으로 높은 점수가 유지되며, 특히 Body는 전원 만점(2점)으로 변별력이 거의 없다.

- Intermediate 으로 채점 시, Grammar, Body, Conclusion 점수가 크게 하락하며 난이도 상승이 반영된다.

- Advanced 으로 채점 시, 모든 항목이 평균 1점 이하로 떨어지며, Grammar는 특히 평균 0.32로 급격히 감소한다.

- Expert 으로 채점 시, Introduction(0.15), Body(0.10), Conclusion(0.03) 점수가 거의 전부 0점에 수렴한다. 이는 Rubric 혹은 Prompt 설계 상 Expert 기준이 지나치게 보수적임을 시사한다.


### 점수 분포

#### BASIC
| 항목           | 0점 | 1점 | 2점 | 평균   |
| ------------ | -- | -- | -- | ---- |
| Grammar      | -  | 22 | 18 | 1.45 |
| Introduction | -  | 2  | 38 | 1.95 |
| Body         | -  | -  | 40 | 2.00 |
| Conclusion   | 6  | 5  | 29 | 1.58 |

#### INTERMEDIATE
| 항목           | 0점 | 1점 | 2점 | 평균   |
| ------------ | -- | -- | -- | ---- |
| Grammar      | 1  | 37 | 2  | 1.03 |
| Introduction | -  | 27 | 13 | 1.33 |
| Body         | -  | 38 | 2  | 1.05 |
| Conclusion   | 5  | 33 | 2  | 0.93 |

#### ADVANCED
| 항목           | 0점 | 1점 | 2점 | 평균   |
| ------------ | -- | -- | -- | ---- |
| Grammar      | 27 | 10 | 1  | 0.32 |
| Introduction | 4  | 34 | -  | 0.89 |
| Body         | 6  | 32 | -  | 0.84 |
| Conclusion   | 7  | 31 | -  | 0.82 |


#### EXPERT
| 항목           | 0점 | 1점 | 2점 | 평균   |
| ------------ | -- | -- | -- | ---- |
| Grammar      | 16 | 22 | 1  | 0.62 |
| Introduction | 33 | 6  | -  | 0.15 |
| Body         | 35 | 4  | -  | 0.10 |
| Conclusion   | 38 | 1  | -  | 0.03 |


### 4.2 항목별 난이도 차이

항목별 점수 분포는 학생 성취도가 아니라 Prompt 기반 체점 로직이 어떻게 작동하는지를 반영한 결과임.  
이를 통해 항목별 난이도 설정과 Prompt 설계 방향성을 파악할 수 있음.

#### Grammar
- Basic: 평균 1.45, 1점과 2점이 균형적으로 분포  
- Intermediate: 평균 1.03, 대부분 1점  
- Advanced: 평균 0.32, 다수 0점  
- Expert: 평균 0.62, 0점과 1점 위주 분포  

→ Grammar 항목은 레벨 상승에 따라 안정적으로 점수가 하락하며, 가장 체계적인 변별력을 제공하는 영역임.  
Prompt가 문법 오류 검출을 단계별 난이도에 맞게 반영하고 있음을 보여주는 사례임.

#### Introduction
- Basic: 평균 1.95, 대부분 2점  
- Intermediate: 평균 1.33, 1점 비중 증가  
- Advanced: 평균 0.89, 주로 1점 분포  
- Expert: 평균 0.15, 대부분 0점  

→ Introduction 항목은 초중급 단계에서는 적절한 분포를 보이지만, Expert 단계에서 급격한 0점 편중 현상이 나타남.  
Prompt가 서론 평가 시 특정 기준 충족 여부에 과도하게 의존하는 경향을 드러내는 지표임.

#### Body
- Basic: 평균 2.00, 전원 만점  
- Intermediate: 평균 1.05, 대부분 1점  
- Advanced: 평균 0.84, 0점·1점 위주 분포  
- Expert: 평균 0.10, 대부분 0점  

→ Body 항목은 Basic 단계에서는 관대한 채점으로 전원 만점을 제공하지만, 상위 레벨에서는 점수가 급격히 감소함.  
Prompt 설계가 본문 평가에서 단순 이분법적 판단에 의존하고 있음을 나타내는 패턴임.

#### Conclusion
- Basic: 평균 1.58, 0~2점이 고르게 분포  
- Intermediate: 평균 0.93, 대부분 1점  
- Advanced: 평균 0.82, 0점·1점 위주 분포  
- Expert: 평균 0.03, 대부분 0점  

→ Conclusion 항목은 모든 레벨에서 가장 높은 변별력을 제공하는 영역임.  
특히 Expert 단계에서는 결론의 형식적 요건 미충족 시 점수가 부여되지 않는 경향을 확인할 수 있음.  
이는 Prompt가 결론 항목에서 강력한 평가 기준을 적용하고 있음을 보여주는 결과임.

---

### 종합 해석
- Grammar 항목은 레벨별 난이도 반영이 가장 성공적으로 이루어진 영역임.  
- Introduction·Body·Conclusion 항목은 초급 단계에서는 합리적인 분포를 보이지만, 상위 레벨로 갈수록 Prompt 기준이 지나치게 가혹하게 작동하는 경향을 확인할 수 있음.  
- 이는 Prompt 설계가 문법 평가에는 안정적으로 작동하나, 구조 평가에서는 편중된 결과를 유발하고 있음을 시사함.  
- 따라서 향후 개선 방향은 문법 기준의 안정성을 유지하면서, 구조 항목에 대해서는 조건 완화 및 세분화를 통해 평가 분포의 균형성을 확보하는 것임.



### 4.3 오류
- submet text에서 json이 받지 못하는 특수 문자 ASCII 가 섞여잇는경우 Fail Calls 발생 

예) \x00, \u0000, \uD800 등


---

## 5. 성능 리포트

* 평균 처리 시간: 9,324 ms
* 요청당 API 비용: $0.0222 (gpt-5-mini 기준)
* 점수 분포: 레벨 상승에 따라 평균 점수 점진적 감소

| Prompt_Version | Avg_Latency (ms) | Avg_Cost (USD) | 샘플 개수 |
|----------------|------------------|----------------|-----------|
| v1.0.0         | 16,353           | 0.00372        | 32        |
| v1.1.0         | 9,592            | 0.00220        | 31        |
| v1.2.0         | 9,269            | 0.00216        | 37        |
| v1.3.0         | 10,272           | 0.00206        | 32        |
| v1.4.0         | 11,863           | 0.00240        | 32        |
| v1.4.1         | 10,407           | 0.00232        | 32        |
| **v1.5.0**     | **9,324**        | **0.0222**     | 32        |


-v1.0.0 : 작동을 위한 기초 prompt
-v1.1.0 : 1차 최적화 진행
-v1.2.0 : gpt 5 mini 에 맞고, 과도한 feedback 방지 
-v1.3.0 : prompt를 가장 경량화 시킴 (체점 결과가 80%가 2점이 방향이 되는 이슈 존재)
-v1.4.0 : v1.2.0을 조금더 경량화 시킴 (채점 결과 score에서 basic에서도 0이 약 70% 로 과한 체점이 이루어짐)
-v1.4.1 : v11.4.0의 문장 순서 수정 및 강제성 주입 (점수 분포는 1.2.0과 유사하나, latency 및 가격 측면에서 grammer 을 제외하고 v1.2.0이 우세)
-v1.5.0 : grammer은 v1.4.1 채용 , 나머지는 v1.2.0 채용

---

## 6. 개선 제안

1. **Prompt 개선**: Expert의 체점 강도 조절. 

2. **Langfuse 활용도**: Langfuse에 대해 study 시간이 부족하여 기능 일부만 사용 및 효율적이지 못한 사용.

1.Langfuse에 대해 처음 사용해보아 이해도 부족, study 시간 부족으로 전반적인 langfuse 을 활용한 버전 관리 미흡 
-> langfuse 의 log -> 다운로드 받아 파싱하여서 prompt 성능 평가진행 등. langfuse 활용도 향상 개선 필요.

3. 물리적 시간 부족으로 대량의 테스트를 통한 최적화 개선 필요.
**prompt**:** 
- prompt version에 대한 평가를 각 rubric_level 당 8회 진행 한 결과 기반으로 테스트 등 통계적 신뢰성이 낮은 상태에서 판단으로 판단의 신뢰성 저하 [유의미한 시간 차이지만, 대표성은 띄지 못함]
- prompt의 micro tuning을 하지 못하여 최적화 및 난의도 조절 미흡

**다양한 파이프라인 시도**
- prompt 에서 1차 intro라 판단한 사항을 body에서는 제외하고 진행, 이후 남은 text 만 conclusion에서 진행과 같이 caching을 통한 방법 시도.
- gpt5 nano 같은 소형 llm을 통해 intro, body, conclusion 분리 후 실행 등의 방식을 진행하지 못함.

3. 후처리 로직 부족
- Scoring, correction, feedback에 관련하여 추가 rephrase, scoring rule의 부재로 0,1,2 int로만 처리: 기획 또는 현업과의 소통을 통해 발전 필요.
- Post-Evaluate에서 레벨 그룹 가중치 및 정합성 검증에서   nlp활용한 score valdiation을 통한 fallback 등 처리방안 추가 
- CEFR, 문장길이를 점수에 활용 등

---

## 7. 결론

본 프로젝트는 Rubric 기반 자동 에세이 평가 시스템을 구현하고, Prompt 버전 관리 및 Langfuse 추적을 통해 실험 가능한 환경을 확보함.

실험 결과, 레벨 상승에 따른 점수 하락이라는 기대된 패턴을 확인하였고, Grammar 항목은 안정적으로 작동했음. 반면 Introduction·Body·Conclusion 항목에서는 상위 레벨에서 편중된 결과가 발생하여 개선 필요성이 도출됨.

향후에는 Prompt 설계의 균형 조정, Langfuse 기반 성능 분석 자동화, 후처리 강화를 통해 보다 교육적으로 활용 가능한 평가 시스템으로 발전 가능성을 확보함.


---
