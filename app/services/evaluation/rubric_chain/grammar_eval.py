import json
import logging
from typing import Any, Dict, Optional

from app.client.bootstrap import build_llm
from app.models.rubric import RubricItemResult
from app.utils.prompt_loader import PromptLoader
from app.utils.tracer import LLM

logger = logging.getLogger(__name__)


class GrammarEvaluator:
    """ë¬¸ë²• ê²€ìˆ˜ë¥¼ ìœ„í•œ í‰ê°€ì í´ë˜ìŠ¤"""

    def __init__(self, client: Optional[LLM] = None, loader: Optional[PromptLoader] = None) -> None:
        self.client = client or build_llm()
        # Use provided PromptLoader if given; otherwise create a new one.
        self.prompt_loader = loader or PromptLoader()

    def _get_grammar_schema(self) -> Dict[str, Any]:
        """ë¬¸ë²• ê²€ìˆ˜ ê²°ê³¼ë¥¼ ìœ„í•œ JSON ìŠ¤í‚¤ë§ˆ (Pydanticì—ì„œ ìë™ ìƒì„±)"""
        return RubricItemResult.model_json_schema()

    async def check_grammar(self, text: str, level: str = "Basic") -> Dict[str, Any]:
        """
        í…ìŠ¤íŠ¸ì˜ ë¬¸ë²•ì„ ê²€ì‚¬í•©ë‹ˆë‹¤.
        Returns: GrammarRubricResult + ë©”íƒ€ë°ì´í„°(token_usage, evaluation_type)
        """
        logger.info(f"ğŸ“ [GRAMMAR] Starting grammar check for level: {level}, text_length: {len(text)}")
        try:
            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            logger.debug(f"Loading grammar prompt for level: {level}")
            system_message = self.prompt_loader.load_prompt("grammar", level)
            messages = [
                {"role": "system", "content": system_message},
                {"role": "user", "content": text},
            ]

            # Azure OpenAI í˜¸ì¶œ
            logger.info(f"ğŸ¤– [GRAMMAR] Sending request to LLM...")
            response = await self.client.run_azure_openai(
                messages=messages,
                json_schema=self._get_grammar_schema(),
                name="grammar_check",
            )

            content = response["content"]
            usage = response.get("usage", {})
            logger.info(f"ğŸ“¥ [GRAMMAR] Received LLM response - tokens: {usage.get('total_tokens', 'unknown')}")
            
            # ëª¨ë¸ì´ ë¬¸ìì—´ JSONì„ ì¤„ ìˆ˜ë„ ìˆìŒ
            if isinstance(content, str):
                content = json.loads(content)

            # ë¹ˆ content ì²´í¬
            if not content or content == {}:
                logger.error(f"âŒ [GRAMMAR] Empty content received from LLM")
                logger.debug(f"Full response: {response}")
                return {
                    "rubric_item": "grammar",
                    "score": 0,
                    "corrections": [],
                    "feedback": "Grammar evaluation failed - empty response from AI model",
                    "token_usage": usage,
                    "evaluation_type": "grammar_check"
                }

            # Pydantic ê²€ì¦/íŒŒì‹±
            logger.debug(f"Parsing grammar response: {content}")
            parsed = RubricItemResult(**content)
            result = parsed.model_dump()

            # ë©”íƒ€ë°ì´í„° ë¶€ê°€
            result["token_usage"] = usage
            result["evaluation_type"] = "grammar_check"
            
            logger.info(f"âœ… [GRAMMAR] Grammar check completed - score: {result['score']}, corrections: {len(result['corrections'])}")
            return result

        except Exception as exc:  # noqa: BLE001
            logger.error(f"ğŸ’¥ [GRAMMAR] Grammar evaluation FAILED: {type(exc).__name__}: {exc}")
            logger.exception("Full exception details for grammar evaluation")
            return {
                "rubric_item": "grammar",
                "score": 1,  # Give a neutral score instead of 0
                "corrections": [],
                "feedback": "ë¬¸ë²• ê²€ì‚¬ ì¤‘ ê¸°ìˆ ì  ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.",
                "error": str(exc),
                "evaluation_type": "grammar_check",
                "token_usage": {
                    "prompt_tokens": 0,
                    "completion_tokens": 0,
                    "total_tokens": 0,
                },
            }
