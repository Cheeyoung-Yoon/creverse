# %%
import asyncio
import json
import logging
import pandas as pd
import requests
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any
import openpyxl
from openpyxl.styles import Font, PatternFill, Alignment
from openpyxl.utils.dataframe import dataframe_to_rows

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('batch_evaluation.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class EssayBatchEvaluator:
    """ë°°ì¹˜ ì—ì„¸ì´ í‰ê°€ê¸° - ë‹¤ì¤‘ prompt ë²„ì „ ì§€ì›, ì¤‘ê°„ ì €ì¥ ê¸°ëŠ¥ í¬í•¨"""
    
    def __init__(self, api_url: str = "http://localhost:8000/v1/essay-eval", prompt_versions: List[str] = None, checkpoint_file: str = None):
        self.api_url = api_url
        self.levels = ["Basic", "Intermediate", "Advanced", "Expert"]
        self.prompt_versions = prompt_versions or ["v1.2.0", "v1.4.1"]
        self.results = {}
        self.checkpoint_file = checkpoint_file or "batch_evaluation_checkpoint.json"
        self.batch_size = 5  # 5ê°œ API í˜¸ì¶œë§ˆë‹¤ ì €ì¥
        self.progress = {"completed_calls": 0, "total_calls": 0, "current_position": None}
        
        logger.info(f"ğŸ”§ Initialized evaluator with prompt versions: {self.prompt_versions}")
        logger.info(f"ğŸ“Š Total combinations: {len(self.levels)} levels Ã— {len(self.prompt_versions)} versions = {len(self.levels) * len(self.prompt_versions)} per essay")
        logger.info(f"ğŸ’¾ Checkpoint file: {self.checkpoint_file}")
        logger.info(f"ğŸ“¦ Batch size: {self.batch_size} calls per save")
        
    def save_checkpoint(self):
        """í˜„ì¬ ì§„í–‰ ìƒí™©ì„ JSON íŒŒì¼ë¡œ ì €ì¥"""
        checkpoint_data = {
            "progress": self.progress,
            "results": self.results,
            "config": {
                "prompt_versions": self.prompt_versions,
                "levels": self.levels,
                "api_url": self.api_url
            },
            "timestamp": datetime.now().isoformat()
        }
        
        try:
            with open(self.checkpoint_file, 'w', encoding='utf-8') as f:
                json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)
            logger.debug(f"ğŸ’¾ Checkpoint saved: {self.progress['completed_calls']}/{self.progress['total_calls']} calls")
        except Exception as e:
            logger.error(f"âŒ Failed to save checkpoint: {e}")
    
    def load_checkpoint(self) -> bool:
        """ì €ì¥ëœ checkpointê°€ ìˆìœ¼ë©´ ë¡œë“œ"""
        try:
            if not Path(self.checkpoint_file).exists():
                logger.info("ğŸ”„ No existing checkpoint found, starting fresh")
                return False
                
            with open(self.checkpoint_file, 'r', encoding='utf-8') as f:
                checkpoint_data = json.load(f)
            
            self.progress = checkpoint_data.get("progress", {})
            self.results = checkpoint_data.get("results", {})
            
            logger.info(f"âœ… Checkpoint loaded: {self.progress.get('completed_calls', 0)}/{self.progress.get('total_calls', 0)} calls completed")
            logger.info(f"ğŸ“… Checkpoint timestamp: {checkpoint_data.get('timestamp', 'unknown')}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ Failed to load checkpoint: {e}")
            return False
    
    def should_skip_call(self, level: str, version: str, essay_idx: int) -> bool:
        """ì´ë¯¸ ì™„ë£Œëœ í˜¸ì¶œì¸ì§€ í™•ì¸"""
        key = f"{level}_{version}"
        if key not in self.results:
            return False
            
        # í•´ë‹¹ ì—ì„¸ì´ê°€ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆëŠ”ì§€ í™•ì¸
        existing_results = self.results[key]
        for result in existing_results:
            if result.get("essay_id") == essay_idx or result.get("essay_index") == essay_idx:
                return True
        return False
        
    def load_sample_data(self, excel_path: str) -> pd.DataFrame:
        """ìƒ˜í”Œ ì—ì„¸ì´ ë°ì´í„° ë¡œë“œ"""
        try:
            df = pd.read_excel(excel_path)
            # ì²« ë²ˆì§¸ í–‰ì´ í—¤ë”ì¸ì§€ í™•ì¸í•˜ê³  NaN ê°’ì´ ìˆëŠ” í–‰ ì œê±°
            df = df.dropna(subset=['submit_text'])
            
            logger.info(f"âœ… Loaded {len(df)} essays from {excel_path}")
            logger.info(f"ğŸ“Š Columns: {list(df.columns)}")
            logger.info(f"ğŸ“‹ Original levels distribution:")
            if 'rubric_level' in df.columns:
                level_counts = df['rubric_level'].value_counts()
                for level, count in level_counts.items():
                    logger.info(f"   {level}: {count} essays")
            
            return df
        except Exception as e:
            logger.error(f"âŒ Failed to load Excel file: {e}")
            raise
    
    def call_evaluation_api(self, essay_text: str, topic_prompt: str, level_group: str, prompt_version: str = "v1.4.1") -> Dict[str, Any]:
        """API í˜¸ì¶œí•˜ì—¬ ì—ì„¸ì´ í‰ê°€"""
        payload = {
            "rubric_level": level_group,
            "topic_prompt": topic_prompt,
            "submit_text": essay_text,
            "prompt_version": prompt_version
        }
        
        try:
            response = requests.post(
                self.api_url,
                json=payload,
                headers={"Content-Type": "application/json"},
                timeout=60  # 60ì´ˆ íƒ€ì„ì•„ì›ƒ
            )
            
            if response.status_code == 200:
                result = response.json()
                logger.debug(f"âœ… API call successful for level {level_group}")
                return {
                    "status": "success",
                    "data": result,
                    "response_time": response.elapsed.total_seconds()
                }
            else:
                logger.error(f"âŒ API call failed with status {response.status_code}")
                return {
                    "status": "error",
                    "error": f"HTTP {response.status_code}: {response.text}",
                    "response_time": response.elapsed.total_seconds()
                }
                
        except requests.exceptions.Timeout:
            logger.error("âŒ API call timed out")
            return {"status": "timeout", "error": "Request timed out"}
        except Exception as e:
            logger.error(f"âŒ API call failed: {e}")
            return {"status": "error", "error": str(e)}
    
    def process_all_essays(self, df: pd.DataFrame) -> Dict[str, List[Dict]]:
        """ëª¨ë“  ì—ì„¸ì´ë¥¼ ëª¨ë“  ë ˆë²¨ê³¼ prompt ë²„ì „ìœ¼ë¡œ í‰ê°€ (checkpoint ì§€ì›)"""
        
        # checkpoint ë¡œë“œ ì‹œë„
        checkpoint_loaded = self.load_checkpoint()
        
        # ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ì´ˆê¸°í™” (checkpointì—ì„œ ë¡œë“œë˜ì§€ ì•Šì€ ê²½ìš°)
        if not checkpoint_loaded:
            self.results = {}
            for level in self.levels:
                for version in self.prompt_versions:
                    key = f"{level}_{version}"
                    self.results[key] = []
        else:
            # checkpointì—ì„œ ë¡œë“œëœ ê²½ìš°, ëˆ„ë½ëœ í‚¤ë“¤ì„ ì´ˆê¸°í™”
            for level in self.levels:
                for version in self.prompt_versions:
                    key = f"{level}_{version}"
                    if key not in self.results:
                        self.results[key] = []
                        logger.info(f"ğŸ”§ Initialized missing key: {key}")
        
        # í˜„ì¬ ê²°ê³¼ ìƒíƒœ ë¡œê·¸
        logger.info("ğŸ“Š Current results status:")
        for key, results_list in self.results.items():
            logger.info(f"  {key}: {len(results_list)} completed essays")
        
        total_calls = len(df) * len(self.levels) * len(self.prompt_versions)
        current_call = self.progress.get("completed_calls", 0)
        
        # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
        self.progress["total_calls"] = total_calls
        
        logger.info(f"ğŸš€ Starting batch evaluation: {len(df)} essays Ã— {len(self.levels)} levels Ã— {len(self.prompt_versions)} versions = {total_calls} API calls")
        if checkpoint_loaded:
            logger.info(f"ğŸ”„ Resuming from checkpoint: {current_call}/{total_calls} calls already completed")
        
        # ì´ˆê¸° ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ì‹œì‘ ì‹œì )
        self.save_checkpoint()
        logger.info(f"ğŸ’¾ Initial checkpoint saved")
        
        try:
            for level in self.levels:
                for version in self.prompt_versions:
                    logger.info(f"ğŸ“Š Processing level: {level}, version: {version}")
                    
                    for idx, row in df.iterrows():
                        # ì´ë¯¸ ì™„ë£Œëœ í˜¸ì¶œì¸ì§€ í™•ì¸
                        if self.should_skip_call(level, version, idx):
                            logger.debug(f"â­ï¸ Skipping already completed: essay {idx}, level {level}, version {version}")
                            continue
                        
                        current_call += 1
                    essay_text = str(row.get('submit_text', ''))
                    essay_id = row.get('essay_id', idx)
                    original_level = row.get('rubric_level', 'unknown')
                    topic_prompt = row.get('topic_prompt', '')
                    
                    if not essay_text or essay_text.strip() == '':
                        logger.warning(f"âš ï¸ Empty essay text at row {idx} (essay_id: {essay_id})")
                        continue
                    
                    logger.info(f"[{current_call}/{total_calls}] Evaluating essay {essay_id} (original: {original_level}) with level {level}, version {version}")
                    logger.debug(f"ğŸ“ Essay preview: {essay_text[:100]}...")
                    
                    # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
                    self.progress["completed_calls"] = current_call
                    self.progress["current_position"] = {
                        "level": level,
                        "version": version,
                        "essay_idx": idx,
                        "essay_id": essay_id
                    }
                    
                    # API í˜¸ì¶œ
                    api_result = self.call_evaluation_api(essay_text, topic_prompt, level, version)
                    
                    # ê²°ê³¼ ì •ë¦¬
                    # ê¸°ë³¸ ì •ë³´ ê¸°ë¡
                    result_record = {
                        "essay_id": essay_id,
                        "original_level": original_level,
                        "evaluation_level": level,
                        "prompt_version": version,  # prompt ë²„ì „ ì •ë³´ ì¶”ê°€
                        "topic_prompt": topic_prompt,
                        "essay_text": essay_text[:500] + "..." if len(essay_text) > 500 else essay_text,  # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ
                        "essay_length": len(essay_text),
                        "response_time": api_result.get("response_time", 0),
                        "api_status": api_result.get("status", "unknown")
                    }
                
                if api_result["status"] == "success":
                    eval_data = api_result["data"]
                    
                    # ì „ì²´ í‰ê°€ ì •ë³´ ì¶”ê°€
                    # if "aggregated" in eval_data:
                    #     agg_data = eval_data["aggregated"]
                    #     result_record["total_score"] = agg_data.get("score", 0)
                    #     result_record["total_corrections_count"] = len(agg_data.get("corrections", []))
                    
                    # grammar ì„¹ì…˜ ì²˜ë¦¬
                    if "grammar" in eval_data:
                        grammar = eval_data["grammar"]
                        result_record["grammar_score"] = grammar.get("score", 0)
                        result_record["grammar_feedback"] = grammar.get("feedback", "")[:500]
                        result_record["grammar_corrections_count"] = len(grammar.get("corrections", []))
                        
                        corrections = grammar.get("corrections", [])
                        if corrections:
                            first_correction = corrections[0]
                            result_record["grammar_first_correction"] = f"{first_correction.get('highlight', '')} â†’ {first_correction.get('correction', '')}"[:200]
                    
                    # structure ì•ˆì˜ ì„¹ì…˜ë“¤ ì²˜ë¦¬
                    if "structure" in eval_data:
                        structure_data = eval_data["structure"]
                        for section_name in ["introduction", "body", "conclusion"]:
                            if section_name in structure_data:
                                section = structure_data[section_name]
                                result_record[f"{section_name}_score"] = section.get("score", 0)
                                result_record[f"{section_name}_feedback"] = section.get("feedback", "")[:500]
                                result_record[f"{section_name}_corrections_count"] = len(section.get("corrections", []))
                                
                                # ì²« ë²ˆì§¸ correctionë§Œ ê¸°ë¡
                                corrections = section.get("corrections", [])
                                if corrections:
                                    first_correction = corrections[0]
                                    result_record[f"{section_name}_first_correction"] = f"{first_correction.get('highlight', '')} â†’ {first_correction.get('correction', '')}"[:200]
                    
                    # íƒ€ì´ë° ì •ë³´
                    if "timings" in eval_data:
                        result_record["total_processing_time"] = eval_data["timings"].get("total", 0) / 1000  # msë¥¼ ì´ˆë¡œ ë³€í™˜
                        
                else:
                    result_record["error"] = api_result.get("error", "Unknown error")
                
                # ë ˆë²¨ê³¼ ë²„ì „ ì¡°í•© í‚¤ë¡œ ê²°ê³¼ ì €ì¥
                key = f"{level}_{version}"
                result_record["essay_index"] = idx  # checkpointì—ì„œ ì‚¬ìš©í•  ì¸ë±ìŠ¤ ì¶”ê°€
                
                # í‚¤ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ì´ˆê¸°í™”
                if key not in self.results:
                    logger.warning(f"âš ï¸ Key {key} not found in results, initializing")
                    self.results[key] = []
                
                self.results[key].append(result_record)
                logger.debug(f"ğŸ“ Added result for {key}, total count: {len(self.results[key])}")
                
                # ë°°ì¹˜ ë‹¨ìœ„ë¡œ checkpoint ì €ì¥
                if current_call % self.batch_size == 0:
                    self.save_checkpoint()
                    logger.info(f"ğŸ’¾ Checkpoint saved at {current_call}/{total_calls} calls")
                    # í˜„ì¬ ê²°ê³¼ ê°œìˆ˜ ë¡œê·¸
                    for k, v in self.results.items():
                        logger.info(f"  {k}: {len(v)} results")
                
                        # API í˜¸ì¶œ ê°„ê²© (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                        time.sleep(1)
        
        except KeyboardInterrupt:
            logger.info("\nâ¹ï¸ Process interrupted by user")
            self.save_checkpoint()
            logger.info(f"ğŸ’¾ Progress saved in checkpoint: {self.checkpoint_file}")
            raise
        
        # ìµœì¢… checkpoint ì €ì¥
        self.save_checkpoint()
        logger.info("âœ… Batch evaluation completed!")
        return self.results
    
    def create_excel_report(self, results: Dict[str, List[Dict]], output_path: str):
        """ê²°ê³¼ë¥¼ ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥ - ë ˆë²¨ë³„, ë²„ì „ë³„ ì‹œíŠ¸ ìƒì„±"""
        logger.info(f"ğŸ“ Creating Excel report: {output_path}")
        
        # ë””ë²„ê¹…: ì‚¬ìš© ê°€ëŠ¥í•œ í‚¤ë“¤ ì¶œë ¥
        logger.info(f"ğŸ” Available result keys: {list(results.keys())}")
        for key, value in results.items():
            logger.info(f"  {key}: {len(value)} results")
        
        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
            # ê° ë ˆë²¨ê³¼ ë²„ì „ ì¡°í•©ì— ëŒ€í•œ ì‹œíŠ¸ ìƒì„±
            for level in self.levels:
                for version in self.prompt_versions:
                    # í‚¤ ê²€ìƒ‰ ì‹œë„ (ì—¬ëŸ¬ í˜•ì‹ìœ¼ë¡œ)
                    possible_keys = [
                        f"{level}_{version}",  # ì›ë³¸ í˜•ì‹ (ì˜ˆ: Basic_v1.5.0)
                        f"{level}_{version.replace('.', '_')}",  # ì ì„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ (ì˜ˆ: Basic_v1_5_0)
                    ]
                    
                    level_version_results = []
                    found_key = None
                    
                    for key in possible_keys:
                        if key in results:
                            level_version_results = results[key]
                            found_key = key
                            logger.info(f"âœ“ Found data for key: {key}")
                            break
                    
                    if not level_version_results:
                        logger.warning(f"âš ï¸ No results for level {level}, version {version}")
                        logger.warning(f"   Tried keys: {possible_keys}")
                        continue
                    
                    # DataFrame ìƒì„±
                    df = pd.DataFrame(level_version_results)
                    
                    # ì‹œíŠ¸ ì´ë¦„ ì„¤ì • (Excel ì‹œíŠ¸ëª… ê¸¸ì´ ì œí•œ ê³ ë ¤, ì ì„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€ê²½)
                    sheet_name = f"{level}_{version.replace('.', '_')}"[:31]  # Excel ì‹œíŠ¸ëª… ìµœëŒ€ 31ì
                    
                    logger.info(f"ğŸ“Š Creating sheet '{sheet_name}' using key '{found_key}' with {len(level_version_results)} results")
                    
                    # ì‹œíŠ¸ì— ì €ì¥
                    df.to_excel(writer, sheet_name=sheet_name, index=False)
                    
                    # ìŠ¤íƒ€ì¼ë§
                    workbook = writer.book
                    worksheet = writer.sheets[sheet_name]
                    
                    # í—¤ë” ìŠ¤íƒ€ì¼ë§
                    header_font = Font(bold=True, color="FFFFFF")
                    header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
                    
                    for cell in worksheet[1]:
                        cell.font = header_font
                        cell.fill = header_fill
                        cell.alignment = Alignment(horizontal="center")
                    
                    # ì—´ ë„ˆë¹„ ìë™ ì¡°ì •
                    for column in worksheet.columns:
                        max_length = 0
                        column = [cell for cell in column]
                        for cell in column:
                            try:
                                if len(str(cell.value)) > max_length:
                                    max_length = len(str(cell.value))
                            except:
                                pass
                        adjusted_width = min(max_length + 2, 50)  # ìµœëŒ€ 50ì
                        worksheet.column_dimensions[column[0].column_letter].width = adjusted_width
                    
                    logger.info(f"âœ… Created sheet: {sheet_name} with {len(level_version_results)} records")
            
            # ìš”ì•½ ì‹œíŠ¸ ìƒì„±
            self.create_summary_sheet(writer, results)
        
        logger.info(f"âœ… Excel report saved: {output_path}")
    
    def create_summary_sheet(self, writer, results: Dict[str, List[Dict]]):
        """ìš”ì•½ ì‹œíŠ¸ ìƒì„± - ë ˆë²¨ë³„, ë²„ì „ë³„ í†µê³„"""
        summary_data = []
        
        for level in self.levels:
            for version in self.prompt_versions:
                key = f"{level}_{version}"
                level_version_results = results.get(key, [])
                
                if not level_version_results:
                    continue
                
                successful_calls = len([r for r in level_version_results if r.get("api_status") == "success"])
                failed_calls = len([r for r in level_version_results if r.get("api_status") != "success"])
                avg_response_time = sum([r.get("response_time", 0) for r in level_version_results]) / len(level_version_results)
                
                # í‰ê·  ì ìˆ˜ ê³„ì‚° (ì„±ê³µí•œ í˜¸ì¶œë§Œ)
                successful_results = [r for r in level_version_results if r.get("api_status") == "success"]
                avg_scores = {}
                
                if successful_results:
                    sections = ["introduction", "body", "conclusion", "grammar"]
                    for section in sections:
                        scores = [r.get(f"{section}_score", 0) for r in successful_results if f"{section}_score" in r]
                        avg_scores[f"avg_{section}_score"] = sum(scores) / len(scores) if scores else 0
                
                summary_record = {
                    "Level": level,
                    "Prompt_Version": version,
                    "Total_Essays": len(level_version_results),
                    "Successful_Calls": successful_calls,
                    "Failed_Calls": failed_calls,
                    "Success_Rate": f"{(successful_calls/len(level_version_results)*100):.1f}%" if level_version_results else "0%",
                    "Avg_Response_Time": f"{avg_response_time:.2f}s",
                    **avg_scores
                }
                
                summary_data.append(summary_record)
        
        summary_df = pd.DataFrame(summary_data)
        summary_df.to_excel(writer, sheet_name="Summary", index=False)
        
        # ìš”ì•½ ì‹œíŠ¸ ìŠ¤íƒ€ì¼ë§
        workbook = writer.book
        worksheet = writer.sheets["Summary"]
        
        header_font = Font(bold=True, color="FFFFFF")
        header_fill = PatternFill(start_color="C55A5A", end_color="C55A5A", fill_type="solid")
        
        for cell in worksheet[1]:
            cell.font = header_font
            cell.fill = header_fill
            cell.alignment = Alignment(horizontal="center")
        
        logger.info("âœ… Created Summary sheet")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Essay Batch Evaluation with Multiple Prompt Versions")
    parser.add_argument("--versions", 
                       default="v1.5.0",
                       help="Comma-separated list of prompt versions (default: v1.4.0,v1.0.0,v1.1.0,v1.3.0)")
    parser.add_argument("--data", 
                       default="../data/essay_writing_40_sample.xlsx",
                       help="Path to input Excel file")
    parser.add_argument("--output", 
                       default="batch_evaluation_comparison.xlsx",
                       help="Output Excel file name")
    parser.add_argument("--checkpoint", 
                       default="batch_evaluation_checkpoint.json",
                       help="Checkpoint file for resume capability")
    parser.add_argument("--batch-size", 
                       type=int,
                       default=5,
                       help="Number of API calls before saving checkpoint (default: 5)")
    parser.add_argument("--resume", 
                       action="store_true",
                       help="Resume from existing checkpoint if available")
    
    args = parser.parse_args()
    
    # prompt ë²„ì „ íŒŒì‹±
    prompt_versions = [v.strip() for v in args.versions.split(",")]
    
    # ê²½ë¡œ ì„¤ì •
    data_path = args.data
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_path = f"comparison_{timestamp}_{args.output}"
    
    # ë°°ì¹˜ í‰ê°€ê¸° ì´ˆê¸°í™” (checkpoint íŒŒì¼ê³¼ í•¨ê»˜)
    evaluator = EssayBatchEvaluator(
        prompt_versions=prompt_versions,
        checkpoint_file=args.checkpoint
    )
    evaluator.batch_size = args.batch_size
    
    try:
        logger.info(f"ğŸš€ Starting batch evaluation comparison")
        logger.info(f"ğŸ“ Prompt versions: {prompt_versions}")
        logger.info(f"ğŸ“ Data file: {data_path}")
        logger.info(f"ğŸ“ Output file: {output_path}")
        logger.info(f"ğŸ’¾ Checkpoint file: {args.checkpoint}")
        logger.info(f"ğŸ“¦ Batch size: {args.batch_size}")
        if args.resume:
            logger.info("ğŸ”„ Resume mode enabled")
        
        # 1. ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ
        logger.info("ğŸ”„ Loading sample data...")
        df = evaluator.load_sample_data(data_path)
        
        # 2. ë°°ì¹˜ í‰ê°€ ì‹¤í–‰
        logger.info("ğŸ”„ Starting batch evaluation...")
        results = evaluator.process_all_essays(df)
        
        # 3. ì—‘ì…€ ë³´ê³ ì„œ ìƒì„±
        logger.info("ğŸ”„ Creating Excel report...")
        evaluator.create_excel_report(results, output_path)
        
        # 4. Checkpoint íŒŒì¼ ì •ë¦¬ (ì™„ë£Œ í›„)
        if Path(args.checkpoint).exists():
            Path(args.checkpoint).unlink()
            logger.info(f"ğŸ—‘ï¸ Removed checkpoint file: {args.checkpoint}")
        
        logger.info("ğŸ‰ Batch evaluation comparison completed successfully!")
        logger.info(f"ğŸ“Š Results saved to: {output_path}")
        
    except KeyboardInterrupt:
        logger.info("\nâ¹ï¸ Evaluation interrupted by user")
        # ì¸í„°ëŸ½íŠ¸ ì‹œì—ë„ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        if 'evaluator' in locals():
            evaluator.save_checkpoint()
        logger.info(f"ğŸ’¾ Progress saved in checkpoint: {args.checkpoint}")
        logger.info("ğŸ”„ Resume with: python excel_creation.py --resume")
    except Exception as e:
        logger.error(f"âŒ Batch evaluation failed: {e}")
        logger.info(f"ğŸ’¾ Progress may be saved in checkpoint: {args.checkpoint}")
        raise
        
        logger.info("ğŸ‰ Batch evaluation comparison completed successfully!")
        logger.info(f"ğŸ“Š Results saved to: {output_path}")
        
    except Exception as e:
        logger.error(f"âŒ Batch evaluation failed: {e}")
        raise
        raise

if __name__ == "__main__":
    main()