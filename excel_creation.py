# %%
"""
Essay Evaluation Batch Processing Script

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” 40ê°œ ìƒ˜í”Œ ì—ì„¸ì´ë¥¼ 4ê°œ ë ˆë²¨(Basic, Intermediate, Advanced, Expert)ë¡œ 
ê°ê° í‰ê°€í•˜ì—¬ ì´ 160ê°œì˜ ê²°ê³¼ë¥¼ ì—‘ì…€ íŒŒì¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
"""

import asyncio
import json
import logging
import pandas as pd
import requests
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any
import openpyxl
from openpyxl.styles import Font, PatternFill, Alignment
from openpyxl.utils.dataframe import dataframe_to_rows

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('batch_evaluation.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class EssayBatchEvaluator:
    """ë°°ì¹˜ ì—ì„¸ì´ í‰ê°€ê¸°"""
    
    def __init__(self, api_url: str = "http://localhost:8000/v1/essay-eval"):
        self.api_url = api_url
        self.levels = ["Basic", "Intermediate", "Advanced", "Expert"]
        self.results = {}
        
    def load_sample_data(self, excel_path: str) -> pd.DataFrame:
        """ìƒ˜í”Œ ì—ì„¸ì´ ë°ì´í„° ë¡œë“œ"""
        try:
            df = pd.read_excel(excel_path)
            # ì²« ë²ˆì§¸ í–‰ì´ í—¤ë”ì¸ì§€ í™•ì¸í•˜ê³  NaN ê°’ì´ ìˆëŠ” í–‰ ì œê±°
            df = df.dropna(subset=['submit_text'])
            
            logger.info(f"âœ… Loaded {len(df)} essays from {excel_path}")
            logger.info(f"ğŸ“Š Columns: {list(df.columns)}")
            logger.info(f"ğŸ“‹ Original levels distribution:")
            if 'rubric_level' in df.columns:
                level_counts = df['rubric_level'].value_counts()
                for level, count in level_counts.items():
                    logger.info(f"   {level}: {count} essays")
            
            return df
        except Exception as e:
            logger.error(f"âŒ Failed to load Excel file: {e}")
            raise
    
    def call_evaluation_api(self, essay_text: str, topic_prompt: str, level_group: str) -> Dict[str, Any]:
        """API í˜¸ì¶œí•˜ì—¬ ì—ì„¸ì´ í‰ê°€"""
        payload = {
            "level_group": level_group,
            "topic_prompt": topic_prompt,
            "submit_text": essay_text
        }
        
        try:
            response = requests.post(
                self.api_url,
                json=payload,
                headers={"Content-Type": "application/json"},
                timeout=60  # 60ì´ˆ íƒ€ì„ì•„ì›ƒ
            )
            
            if response.status_code == 200:
                result = response.json()
                logger.debug(f"âœ… API call successful for level {level_group}")
                return {
                    "status": "success",
                    "data": result,
                    "response_time": response.elapsed.total_seconds()
                }
            else:
                logger.error(f"âŒ API call failed with status {response.status_code}")
                return {
                    "status": "error",
                    "error": f"HTTP {response.status_code}: {response.text}",
                    "response_time": response.elapsed.total_seconds()
                }
                
        except requests.exceptions.Timeout:
            logger.error("âŒ API call timed out")
            return {"status": "timeout", "error": "Request timed out"}
        except Exception as e:
            logger.error(f"âŒ API call failed: {e}")
            return {"status": "error", "error": str(e)}
    
    def process_all_essays(self, df: pd.DataFrame) -> Dict[str, List[Dict]]:
        """ëª¨ë“  ì—ì„¸ì´ë¥¼ ëª¨ë“  ë ˆë²¨ë¡œ í‰ê°€"""
        results = {level: [] for level in self.levels}
        total_calls = len(df) * len(self.levels)
        current_call = 0
        
        logger.info(f"ğŸš€ Starting batch evaluation: {len(df)} essays Ã— {len(self.levels)} levels = {total_calls} API calls")
        
        for level in self.levels:
            logger.info(f"ğŸ“Š Processing level: {level}")
            
            for idx, row in df.iterrows():
                current_call += 1
                essay_text = str(row.get('submit_text', ''))  # ì‹¤ì œ ì»¬ëŸ¼ëª… ì‚¬ìš©
                essay_id = row.get('essay_id', idx)
                original_level = row.get('rubric_level', 'unknown')
                topic_prompt = row.get('topic_prompt', '')
                
                if not essay_text or essay_text.strip() == '':
                    logger.warning(f"âš ï¸ Empty essay text at row {idx} (essay_id: {essay_id})")
                    continue
                
                logger.info(f"[{current_call}/{total_calls}] Evaluating essay {essay_id} (original: {original_level}) with level {level}")
                logger.debug(f"ğŸ“ Essay preview: {essay_text[:100]}...")
                
                # API í˜¸ì¶œ
                api_result = self.call_evaluation_api(essay_text, topic_prompt, level)
                
                # ê²°ê³¼ ì •ë¦¬
                # ê¸°ë³¸ ì •ë³´ ê¸°ë¡
                result_record = {
                    "essay_id": essay_id,
                    "original_level": original_level,
                    "evaluation_level": level,
                    "topic_prompt": topic_prompt,
                    "essay_text": essay_text[:500] + "..." if len(essay_text) > 500 else essay_text,  # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ
                    "essay_length": len(essay_text),
                    "response_time": api_result.get("response_time", 0),
                    "api_status": api_result.get("status", "unknown")
                }
                
                if api_result["status"] == "success":
                    eval_data = api_result["data"]
                    
                    # ì „ì²´ í‰ê°€ ì •ë³´ ì¶”ê°€
                    if "aggregated" in eval_data:
                        agg_data = eval_data["aggregated"]
                        result_record["total_score"] = agg_data.get("score", 0)
                        result_record["total_corrections_count"] = len(agg_data.get("corrections", []))
                    
                    # grammar ì„¹ì…˜ ì²˜ë¦¬
                    if "grammar" in eval_data:
                        grammar = eval_data["grammar"]
                        result_record["grammar_score"] = grammar.get("score", 0)
                        result_record["grammar_feedback"] = grammar.get("feedback", "")[:500]
                        result_record["grammar_corrections_count"] = len(grammar.get("corrections", []))
                        
                        corrections = grammar.get("corrections", [])
                        if corrections:
                            first_correction = corrections[0]
                            result_record["grammar_first_correction"] = f"{first_correction.get('highlight', '')} â†’ {first_correction.get('correction', '')}"[:200]
                    
                    # structure ì•ˆì˜ ì„¹ì…˜ë“¤ ì²˜ë¦¬
                    if "structure" in eval_data:
                        structure_data = eval_data["structure"]
                        for section_name in ["introduction", "body", "conclusion"]:
                            if section_name in structure_data:
                                section = structure_data[section_name]
                                result_record[f"{section_name}_score"] = section.get("score", 0)
                                result_record[f"{section_name}_feedback"] = section.get("feedback", "")[:500]
                                result_record[f"{section_name}_corrections_count"] = len(section.get("corrections", []))
                                
                                # ì²« ë²ˆì§¸ correctionë§Œ ê¸°ë¡
                                corrections = section.get("corrections", [])
                                if corrections:
                                    first_correction = corrections[0]
                                    result_record[f"{section_name}_first_correction"] = f"{first_correction.get('highlight', '')} â†’ {first_correction.get('correction', '')}"[:200]
                    
                    # íƒ€ì´ë° ì •ë³´
                    if "timings" in eval_data:
                        result_record["total_processing_time"] = eval_data["timings"].get("total", 0) / 1000  # msë¥¼ ì´ˆë¡œ ë³€í™˜
                        
                else:
                    result_record["error"] = api_result.get("error", "Unknown error")
                
                results[level].append(result_record)
                
                # API í˜¸ì¶œ ê°„ê²© (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                time.sleep(1)
        
        logger.info("âœ… Batch evaluation completed!")
        return results
    
    def create_excel_report(self, results: Dict[str, List[Dict]], output_path: str):
        """ê²°ê³¼ë¥¼ ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥"""
        logger.info(f"ğŸ“ Creating Excel report: {output_path}")
        
        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
            for level in self.levels:
                level_results = results[level]
                if not level_results:
                    logger.warning(f"âš ï¸ No results for level {level}")
                    continue
                
                # DataFrame ìƒì„±
                df = pd.DataFrame(level_results)
                
                # ì‹œíŠ¸ì— ì €ì¥
                df.to_excel(writer, sheet_name=f"{level}_Results", index=False)
                
                # ìŠ¤íƒ€ì¼ë§
                workbook = writer.book
                worksheet = writer.sheets[f"{level}_Results"]
                
                # í—¤ë” ìŠ¤íƒ€ì¼ë§
                header_font = Font(bold=True, color="FFFFFF")
                header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
                
                for cell in worksheet[1]:
                    cell.font = header_font
                    cell.fill = header_fill
                    cell.alignment = Alignment(horizontal="center")
                
                # ì—´ ë„ˆë¹„ ìë™ ì¡°ì •
                for column in worksheet.columns:
                    max_length = 0
                    column = [cell for cell in column]
                    for cell in column:
                        try:
                            if len(str(cell.value)) > max_length:
                                max_length = len(str(cell.value))
                        except:
                            pass
                    adjusted_width = min(max_length + 2, 50)  # ìµœëŒ€ 50ì
                    worksheet.column_dimensions[column[0].column_letter].width = adjusted_width
                
                logger.info(f"âœ… Created sheet: {level}_Results with {len(level_results)} records")
            
            # ìš”ì•½ ì‹œíŠ¸ ìƒì„±
            self.create_summary_sheet(writer, results)
        
        logger.info(f"âœ… Excel report saved: {output_path}")
    
    def create_summary_sheet(self, writer, results: Dict[str, List[Dict]]):
        """ìš”ì•½ ì‹œíŠ¸ ìƒì„±"""
        summary_data = []
        
        for level in self.levels:
            level_results = results[level]
            if not level_results:
                continue
            
            successful_calls = len([r for r in level_results if r.get("api_status") == "success"])
            failed_calls = len([r for r in level_results if r.get("api_status") != "success"])
            avg_response_time = sum([r.get("response_time", 0) for r in level_results]) / len(level_results)
            
            # í‰ê·  ì ìˆ˜ ê³„ì‚° (ì„±ê³µí•œ í˜¸ì¶œë§Œ)
            successful_results = [r for r in level_results if r.get("api_status") == "success"]
            avg_scores = {}
            
            if successful_results:
                sections = ["introduction", "body", "conclusion", "grammar"]
                for section in sections:
                    scores = [r.get(f"{section}_score", 0) for r in successful_results if f"{section}_score" in r]
                    avg_scores[f"avg_{section}_score"] = sum(scores) / len(scores) if scores else 0
            
            summary_record = {
                "Level": level,
                "Total_Essays": len(level_results),
                "Successful_Calls": successful_calls,
                "Failed_Calls": failed_calls,
                "Success_Rate": f"{(successful_calls/len(level_results)*100):.1f}%" if level_results else "0%",
                "Avg_Response_Time": f"{avg_response_time:.2f}s",
                **avg_scores
            }
            
            summary_data.append(summary_record)
        
        summary_df = pd.DataFrame(summary_data)
        summary_df.to_excel(writer, sheet_name="Summary", index=False)
        
        # ìš”ì•½ ì‹œíŠ¸ ìŠ¤íƒ€ì¼ë§
        workbook = writer.book
        worksheet = writer.sheets["Summary"]
        
        header_font = Font(bold=True, color="FFFFFF")
        header_fill = PatternFill(start_color="C55A5A", end_color="C55A5A", fill_type="solid")
        
        for cell in worksheet[1]:
            cell.font = header_font
            cell.fill = header_fill
            cell.alignment = Alignment(horizontal="center")
        
        logger.info("âœ… Created Summary sheet")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ê²½ë¡œ ì„¤ì •
    data_path = "/home/ycy/work_dir/creverse/data/essay_writing_40_sample.xlsx"
    output_path = "/home/ycy/work_dir/creverse/excel_creation.xlsx"
    
    # ë°°ì¹˜ í‰ê°€ê¸° ì´ˆê¸°í™”
    evaluator = EssayBatchEvaluator()
    
    try:
        # 1. ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ
        logger.info("ğŸ”„ Loading sample data...")
        df = evaluator.load_sample_data(data_path)
        
        # 2. ë°°ì¹˜ í‰ê°€ ì‹¤í–‰
        logger.info("ğŸ”„ Starting batch evaluation...")
        results = evaluator.process_all_essays(df)
        
        # 3. ì—‘ì…€ ë³´ê³ ì„œ ìƒì„±
        logger.info("ğŸ”„ Creating Excel report...")
        evaluator.create_excel_report(results, output_path)
        
        logger.info("ğŸ‰ Batch evaluation completed successfully!")
        logger.info(f"ğŸ“Š Results saved to: {output_path}")
        
    except Exception as e:
        logger.error(f"âŒ Batch evaluation failed: {e}")
        raise

if __name__ == "__main__":
    main()